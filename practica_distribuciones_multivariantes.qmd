---
title: "Distribuciones multivariantes en práctica: Normal y familias relacionadas"
date: "`r format(Sys.Date())`"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
  pdf_document: default
fontsize: 11pt
geometry: margin=1in
embed-resources: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library("tidyverse")
library("palmerpenguins")
library("GGally")
library("MVN")

```

```{r}

# Para recoger los datos: 

install.packages("palmerpenguins", repos = "https://cloud.r-project.org/")
library(palmerpenguins)


```


Considera el conjunto de datos [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/), leed la documentación, instalad y cargad el conjunto de datos penguins

![](pinguinos.png){width=40% style="display:block; margin:auto;"}

Cargamos las librerías, leemos los datos y los resumimos por especie:

```{r data}
library(tidyverse)
library(palmerpenguins)

pinguinos <- palmerpenguins::penguins %>%
  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) 


pinguinos %>%
  group_by(species) %>%
  summarise(
    n = n(),
    across(where(is.numeric), ~ mean(.x, na.rm = TRUE)),
    .groups = "drop"
  )
```


Vamos a trabajar con la especie "Adelie" que es donde tenemos más datos

```{r seleccion-especie}
A <- pinguinos %>% filter(species == "Adelie") %>% select(-species) %>%
  drop_na()
n <- nrow(A); p <- ncol(A)
n; p
```

## Análisis descriptivo multivariante 

```{r resumen}
mu <- colMeans(A)
S  <- cov(A)

round(mu, 2)
round(S, 2)
```

```{r pairs}
GGally::ggpairs(A)
```

### 1. Escribe en 3–4 líneas las asociaciones más claras. ¿Tiene sentido suponer relaciones aproximadamente lineales? ¿Las densidades marginales lucen gaussianas, eso garantiza normalidad multivariante?

Como podemos ver aunque las correlaciones son altamente significativas, es decir podemos extrapolar los datos a la poblacion con una confianza del 95% son bajas por lo que no hay una dependencia lineal clara, cosa que se confirma con los scatterplots. Además las densidades de cada variable se podrían aproximar por una normal, aunque en la segunda es casi bimodal (no se puede aproximar tan bien por una normal) aunque el pico es bajo y no es tan grave. En la última tambien podemos ver que tiene sesgo positivo, ya que baja de manera menos pronunciada en la cola derecha.


  
```{r}
# (X,Y) con X ~ N(0,1) y Y = ±X con prob 1/2

set.seed(123)

n <- 2e4
x <- rnorm(n)
s <- sample(c(-1, 1), n, replace = TRUE)
y <- s * x
df <- data.frame(x, y)

p1 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.25, shape = 16, size = 0.7) +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  geom_abline(slope = -1, intercept = 0, linetype = 2) +
  coord_equal() +
  labs(title = "Densidad conjunta del contraejemplo")

p1
```


La densidad conjunta  es
$$
f_{X,Y}(x,y)=\tfrac12\phi(x)\delta(y-x)+\tfrac12,\phi(x),\delta(y+x),
$$
donde $\phi$ es la densidad normal estándar y $\delta$ es la delta de Dirac.

(Completa los detalles !)

**Recuerda:**
En una normal multivariante:

* Las curvas de nivel son elipses en 2D (elipsoides en dimensiones mayores).
* La dependencia entre variables queda completamente determinada por la matriz de covarianzas $\Sigma$.

Mirar solo las marginales puede ocultar dependencias no lineales o multimodales que rompen la normalidad conjunta.

### Respuesta

Ahora vamos a demostrar que siendo X~N(0,1), Y también lo será. Notemos que la función de densidad f_{X} es la función de densidad de una normal. Además tenemos que:

$$
Y = 
\begin{cases}
 X, & \text{con probabilidad } \tfrac{1}{2}, \\
 -X, & \text{con probabilidad } \tfrac{1}{2}.
\end{cases}
$$

Si utilizamos el Teorema de Canvio de Variable, que es el siguiente:

$$
\textbf{Teorema (Cambio de variable no monótono).} \\
\text{Sea } X \text{ una variable aleatoria continua con densidad } f_X(x),
\text{ y sea } Y = g(X),\\

\text{donde } g \text{ es una función diferenciable que puede no ser monótona.}\\
\text{ Si existen varios valores } x_i \text{ tales que } g(x_i) = y,
\text{ entonces la densidad de } Y \text{ es:}\\

f_Y(y) = \sum_i f_X(x_i) \, \left| \frac{d}{dy} g^{-1}_i(y) \right|.
$$

Notemos que el valor absoluto será siempre uno ya que lo de dentro es +1 o -1. Además al ser la densidad de una normal f_{X}(-x)=f_{X}(x) Por lo que podemos concluir que f_Y(y)=f_X(x), es decir, Y~N(0,1).

Densidad multivariante:

En los pingüinos Adelie, variables como el largo del pico y el largo del aleteo muestran asociaciones positivas, y la masa corporal se relaciona también con estas medidas. Aunque las relaciones parecen aproximadamente lineales y las distribuciones marginales se ven gaussianas, esto no garantiza normalidad multivariante, ya que dependencias no lineales o multimodales pueden romperla. Por ejemplo, en el caso Y = ±X, las marginales son normales pero la densidad conjunta no; si fuera normal multivariante, las curvas de nivel serían elipses, mientras que aquí son dos rectas.



### 2. Interpreta dos términos fuera de la diagonal de $\Sigma$. ¿Qué nos dicen sobre la orientación de las elipses de nivel?

```{r}
S=cov(A)
S
```
Como podemos ver la entrada (1,2) bill_length_mm y bill_depth_mm  la covarianza es positiva, por lo que cuando una aumenta, la otra también, y cuando una disminuye, la otra lo mismo. Lo mismo pasa con (3,4). Pero como vemos la diferencia es muy alta, lo que las curvas de nivel serán mucho mas estiradas en la (3,4) que en la (1,2).

Veamos las curvas de nivel
```{r}
# install.packages("ellipse")  
library(ellipse)

colnames(S) <- rownames(S) <- c("bill_length_mm","bill_depth_mm","flipper_length_mm","body_mass_g")

# Centrar en (0,0) porque no hay datos
centro <- c(0,0)

# Dibujo
plot(1, type="n", xlim=c(-100,100), ylim=c(-5000,5000), 
     xlab="flipper_length_mm", ylab="body_mass_g")
lines(ellipse(S[3:4, 3:4], centre=centro), col="blue", lwd=2)

plot(1, type="n", xlim=c(-10,10), ylim=c(-10,10), 
     xlab="bill_length_mm", ylab="bill_depth_mm")
lines(ellipse(S[1:2, 1:2], centre=centro), col="red", lwd=2)

```
Como se puede ver está mucho mas estirada la azul que la roja.

### 3. Utiliza las distancias de Mahalanobis para detectar datos atípicos multivariantes. ¿Los puntos señalados como atípicos lo son también en sentido univariante? Compara con boxplots univariantes.

La distancia de Mahalanobis mide lo “lejos” que está un punto del centro considerando las escalas y correlaciones entre variables. Para un vector de observaciones $\mathbf{x}\in\mathbb{R}^p$
con media muestral $\boldsymbol{\mu}$ y matriz de covarianzas $\mathbf{S}$, se define
$$D^2(\mathbf{x}) = (\mathbf{x}-\boldsymbol{\mu})^\top \mathbf{S}^{-1} (\mathbf{x}-\boldsymbol{\mu})$$.

Si los datos proceden de una normal multivariante $N_p(\boldsymbol{\mu},\mathbf{S})$, entonces
$D^2(\mathbf{x}) \sim \chi^2_p$. Por tanto, valores grandes de $D^2$ indican posibles atípicos en el sentido multivariante.

*Regla práctica:*

Para cada observación $i$, calcula $D_i^2$ y compárala con el cuantil crítico:
$D_i^2 > \chi^2_{p,\,1-\alpha} \;\Rightarrow\;$ marcar como candidato a atípico
(con $\alpha$ típico $0.05$ o $0.01$).

La distancia de Mahalanobis:

* Estandariza por varianzas (no se ve afectada por escalas distintas), 

* Corrige por correlaciones vía $\mathbf{S}^{-1}$, y 

* Genera elipses de nivel $\{\mathbf{x} : D^2(\mathbf{x}) = c\}$ coherentes con la geometría de $N_p$.

Por eso detecta observaciones que pueden ser normales univariadamente pero atípicas al considerar el vector completo.

```{r}

# Seleccionamos solo las columnas numéricas multivariantes:
X <- A[, c("bill_length_mm", "bill_depth_mm", 
            "flipper_length_mm", "body_mass_g")]

# Media muestral (vector μ)
mu <- colMeans(X)

# Matriz de covarianzas S
S <- cov(X)

# Distancia de Mahalanobis para cada observación
D2 <- mahalanobis(X, mu, S)

D2

```

Ahora calculamos el cuantil de la chisq que es el siguiente: 

```{r}
p <- 4
alpha <- 0.025 # a cada lado hace 0,95 dentro que es lo que queremos

cuantil <- qchisq(1 - alpha, df = p)
cuantil

```

Ahora solo hay que comprobar si la distancia de mahalanobis es mas grande que el cuantil o no en cada caso:

```{r}
which(D2>cuantil)
```

Estas son las entradas que so candidatos a atípico.

Ahora veamos si los puntos señalados como atípicos lo son también en sentido univariante. 

```{r}
boxplot(A$bill_length_mm)
boxplot(A$bill_depth_mm)
boxplot(A$flipper_length_mm)
boxplot(A$body_mass_g)
```


### 4. Evalúa la normalidad multivariante : tests y QQ-plots. ¿Observas curvaturas sistemáticas (no linealidad global) o solo unos pocos puntos extremos? 

En cuanto a los tests, intenta usar la librería: `MVN` prueba "mardia", "hz" y "royston" ¿en qué se basan cada uno?.




```{r}
#install.packages("MVN")

# Mardia (asimetría y curtosis)
result_mardia <- mvn(data = A_num, mvn_test = "mardia")
result_mardia$multivariate_normality
```

```{r}
# Henze-Zirkler
result_hz <- mvn(data = A_num, mvn_test = "hz")
result_hz$multivariate_normality
```

```{r}
# Royston
result_royston <- mvn(data = A_num, mvn_test = "royston")
result_royston$multivariate_normality

```
Hacemos qqplot 

En cuanto al QQ-plot, te dejo un pseudo algoritmo para que lo implementes:

```{r, eval=FALSE}
A_num <- as.matrix(A_num)        # asegurar matriz numérica
mu <- colMeans(A_num)            # recalcular medias del mismo objeto
S <- cov(A_num)                  # recalcular covarianza del mismo objeto

n <- nrow(A_num)
p <- ncol(A_num)

d2 <- mahalanobis(A_num, mu, S)

qqplot(qchisq(ppoints(n), df = p), sort(d2),
       main = "QQ-plot de distancias de Mahalanobis",
       xlab = expression("Cuantiles teóricos" ~ chi^2[p]),
       ylab = expression("Distancias de Mahalanobis" ~ D^2))
abline(0, 1, col = "red", lwd = 2)


```

* Si los puntos caen cerca de la recta: la normalidad multivariante es plausible.

* Si hay una curvatura clara o puntos extremos: hay desviaciones o outliers multivariantes. En ese caso, vuelve a evaluar la normalidad tras excluirlos.


### 5. Comparación entre especies: Superpón elipses normales por especie (elige dos variables) y comenta diferencias de centro y forma/orientación:

```{r}
library(ellipse)
library(palmerpenguins)

# Selección de variables
vars <- c("bill_length_mm","bill_depth_mm",
          "flipper_length_mm","body_mass_g")

# Filtrar casos completos
peng <- na.omit(penguins[, c(vars, "species")])

# Colores por especie
cols <- c("red", "blue", "darkgreen")

# Par de variables a mostrar como ejemplo
xvar <- "bill_length_mm"
yvar <- "bill_depth_mm"

# Plot vacío con límites adecuados
plot(peng[[xvar]], peng[[yvar]],
     type = "n",
     xlab = xvar,
     ylab = yvar,
     main = "Elipses de covarianza (95%) + Cruces en la media")

# Añadir puntos por especie
for(i in 1:3){
  esp <- unique(peng$species)[i]
  data_i <- subset(peng, species == esp)
  
  # Media del grupo
  mu <- colMeans(data_i[, c(xvar, yvar)])
  # Covarianza del grupo
  S  <- cov(data_i[, c(xvar, yvar)])
  
  # Añadir puntos individuales
  points(data_i[[xvar]], data_i[[yvar]], col = cols[i])
  
  # Añadir elipse
  lines(ellipse(S, centre = mu, level = 0.95),
        col = cols[i], lwd = 2)
  
  # Añadir CRUZ en la media
  points(mu[1], mu[2],
         pch = 4,   # cruz
         col = cols[i],
         cex = 2,
         lwd = 2)
}

legend("topleft",
       legend = unique(peng$species),
       col = cols,
       pch = 4, lwd = 2,
       cex = 0.9,
       title = "Especies")

```

el centro es para comparar medias y la dispersion la varianza y concentracion de los puntos.


### 6. ¿Parece razonable asumir una $\Sigma$ común entre especies? Justifica con evidencia gráfica.

Podrías considerar la misma ya que tienen la misma forma el mismo tamaño, aunque esten desplazadas unas de otras.


### 7. ¿Podemos afirmar que la matriz $\mathbf{S}$ de los pinguinos Adelie sigue una distribución de Wishart?



### 8. Comprueba por simulación que $(n-1)\mathbf{S}$, obtenida a partir de muestras de una normal multivariante, sigue aproximadamente una distribución de Wishart. ¿Coinciden las medias empíricas de los elementos de $(n-1)\mathbf{S}$ con los valores esperados según $(n-1)\boldsymbol{\Sigma}$? ¿Qué ocurre si se aumenta o reduce $n$? ¿Y si se cambia la matriz $\boldsymbol{\Sigma}$?

### 9. Genera una muestra de tamaño $n$ y dimensión $p$ de una distribución normal multivariante. Calcula el estadístico de Hotelling $T^2$ y transforma su valor a la escala de una F mediante la relación conocida entre ambas distribuciones. Explora cómo varía el valor de $T^2$ al modificar: el tamaño muestral $n$,la dimensión $p$,la distancia entre el vector de medias y un valor de referencia $\boldsymbol{\mu}_0$. Representa gráficamente el comportamiento de $T^2$ o de su equivalente en la escala F cuando cambias estos parámetros.

### 10. Simula ahora varios grupos con medias distintas pero con la misma matriz de covarianzas, y calcula el valor de Wilks $\Lambda$ y su transformación a F. Usa el siguiente código como base:

```{r}
library(MASS)
# Ejemplo con 3 grupos simulados
set.seed(123)
g <- 3; n <- 25; p <- 2
Sigma <- matrix(c(1, 0.6, 0.6, 1), 2)
mu_list <- list(c(0,0), c(1,0.5), c(0.5,1))
X <- do.call(rbind, lapply(1:g, function(k) mvrnorm(n, mu_list[[k]], Sigma)))
grupo <- factor(rep(1:g, each = n))
D <- data.frame(grupo, X)

# MANOVA (acrónimo de Multivariate Analysis of Variance) es la extensión multivariante del ANOVA
modelo <- manova(cbind(X1, X2) ~ grupo, data = D)
summary(modelo, test = "Wilks")
```

##### 10a. Calcula manualmente las matrices $\mathbf{W}$, $\mathbf{B}$ y $\mathbf{T}$, y el cociente $\Lambda = |\mathbf{W}| / |\mathbf{T}|$.

##### 10b. Comprueba que la F obtenida en la salida de summary(modelo, test = "Wilks") coincide con la transformación aproximada de $\Lambda$.

##### 10c. Repite la simulación con medias más separadas y observa cómo varían $\Lambda$ y F.
